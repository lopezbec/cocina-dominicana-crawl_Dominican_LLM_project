# Dominican Culinary Culture Scraper ðŸ‡©ðŸ‡´

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![API](https://img.shields.io/badge/Firecrawl-API-FF6B6B?style=for-the-badge&logo=fire&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Production-success?style=for-the-badge)

**Production-Ready Web Scraper for Dominican Culinary Heritage**

*Comprehensive content extraction from Cocina Dominicana with canonical logging, clean architecture, and robust error handling*

</div>

## Platform Requirements

> **This application runs Firecrawl locally using Docker Compose - no API key required**

### System Architecture

```mermaid
graph TB
    A[scraper.py] --> B[Firecrawl API:3002]
    B --> C[Redis:6379]
    B --> D[PostgreSQL:5432]
    B --> E[Content Extraction]
    A --> F[File System]
    F --> G[scraped_content/]
    G --> H[Markdown Files]
    G --> I[JSON Metadata]
    G --> J[Summary Report]
    
    style A fill:#3776AB,stroke:#fff,stroke-width:2px,color:#fff
    style B fill:#FF6B6B,stroke:#fff,stroke-width:2px,color:#fff
    style G fill:#28a745,stroke:#fff,stroke-width:2px,color:#fff
```

## Dependencies

### Core Requirements

| Component | Version | Purpose |
|-----------|---------|---------|
| Docker | 20.10+ | Container runtime |
| Docker Compose | 2.0+ | Service orchestration |
| Python | 3.8+ | Runtime environment |
| firecrawl-py | Latest | API client library |
| python-dotenv | Latest | Environment variable management |

### Local Firecrawl Stack
The application runs Firecrawl locally via Docker:

| Service | Port | Purpose |
|---------|------|---------|
| Firecrawl API | 3002 | Web scraping engine |
| Redis | 6379 | Job queue management |
| PostgreSQL | 5432 | Job state persistence |

### Compatibility Matrix

```mermaid
gantt
    title Platform Support
    dateFormat X
    axisFormat %s
    
    section Operating Systems
    macOS     : 0, 1
    Linux     : 0, 1  
    Windows   : 0, 1
    
    section Python Versions
    Python 3.8  : 0, 1
    Python 3.9  : 0, 1
    Python 3.10 : 0, 1
    Python 3.11 : 0, 1
    Python 3.12 : 0, 1
```

## Installation & Setup

### Prerequisites

- **Docker** and **Docker Compose** installed ([Get Docker](https://docs.docker.com/get-docker/))
- **Python 3.8+**
- **4GB RAM minimum** (8GB recommended)

### Quick Start

```bash
git clone https://github.com/cristiandlahoz/cocina-dominicana-crawl.git
cd cocina-dominicana-crawl

python3 -m venv venv
source venv/bin/activate

make setup

make test

make scrape

make firecrawl-stop
```

### Detailed Setup

#### Step 1: Clone and Setup Python Environment

```bash
git clone https://github.com/cristiandlahoz/cocina-dominicana-crawl.git
cd cocina-dominicana-crawl

python3 -m venv venv
source venv/bin/activate
```

#### Step 2: Run Complete Setup

```bash
make setup
```

This command will:
- Install Python dependencies
- Clone and initialize Firecrawl
- Build Docker images (5-10 minutes first time)
- Start all services (API, Redis, PostgreSQL, Playwright)

#### Step 3: Verify Services

```bash
make test
```

Expected: JSON response with scraped content from example.com

#### Step 4: Run Scraper

```bash
make scrape
```

#### Step 5: Stop Services When Done

```bash
make firecrawl-stop
```

### Makefile Commands

View all available commands:

```bash
make help
```

Common commands:

| Command | Description |
|---------|-------------|
| `make setup` | Complete first-time setup |
| `make firecrawl-start` | Start Firecrawl services |
| `make firecrawl-stop` | Stop Firecrawl services |
| `make firecrawl-restart` | Restart all services |
| `make firecrawl-status` | Show service status |
| `make firecrawl-logs` | Follow API logs |
| `make scrape` | Run all configured sections |
| `make scrape-url URL=<url>` | Scrape a single URL |
| `make scrape-category URL=<url>` | Crawl category and scrape articles |
| `make scrape-list FILE=<file>` | Scrape URLs from file |
| `make scrape-discover URL=<url>` | Discover URLs without scraping |
| `make test` | Test Firecrawl endpoint |
| `make clean` | Remove scraped content |
| `make clean-all` | Remove everything including Firecrawl |

## Features

Production-ready scraper with enterprise-grade reliability:

### Core Capabilities

- **Local Firecrawl**: No API keys, no rate limits, full control
- **Flexible Scraping**: Single URL, category crawl, batch processing, or section-based
- **Smart URL Discovery**: Automatic link extraction with configurable filtering
- **Canonical Logging**: Structured logging following Stripe's canonical log pattern
- **Clean Architecture**: Functions follow Single Responsibility Principle with <20 line limit
- **Fast Scraping**: 6x faster than cloud API (0.5s vs 3s per article)
- **Resume Capability**: Automatically skips already scraped articles
- **Performance Monitoring**: Built-in timing for all operations
- **Robust Error Handling**: Comprehensive exception handling with detailed logging
- **File Organization**: Automatic directory structure creation and safe filename generation
- **Docker-based**: Easy setup with docker-compose, no complex configuration
- **CLI Interface**: User-friendly command-line interface with multiple commands
- **Configuration-driven**: YAML-based configuration for easy customization

### Content Categories

The scraper targets 4 main cultural sections:

1. **Cultura y OrÃ­genes** (`cultura_origenes/`)
2. **Tradiciones y Costumbres** (`tradiciones_costumbres/`)  
3. **Festividades y Celebraciones** (`festividades_celebraciones/`)
4. **Comparaciones** (`comparaciones/`)

### Data Model

```mermaid
erDiagram
    ARTICLE {
        string title
        string description
        string url
        string url_slug
        datetime scraped_at
        int word_count
        int char_count
    }
    
    SECTION {
        string name
        string url
        string directory
        int articles_count
    }
    
    SESSION {
        datetime start_time
        datetime end_time
        float duration_seconds
        int total_articles
    }
    
    SECTION ||--o{ ARTICLE : contains
    SESSION ||--o{ SECTION : processes
```

## Usage

### Command-Line Interface

The scraper provides a flexible CLI for different scraping scenarios:

#### Scrape All Configured Sections

```bash
make scrape
python scraper.py
python cli.py scrape-all
```

#### Scrape Single URL

```bash
make scrape-url URL="https://www.cocinadominicana.com/batata-asada"
python cli.py scrape "https://www.cocinadominicana.com/batata-asada"
python cli.py scrape "https://www.cocinadominicana.com/batata-asada" --output recipes
```

#### Crawl Category Page

Automatically discover and scrape all articles from a category page:

```bash
make scrape-category URL="https://www.cocinadominicana.com/cocina" DEPTH=2
python cli.py crawl "https://www.cocinadominicana.com/cocina"
python cli.py crawl "https://www.cocinadominicana.com/cocina" --depth 2 --name cocina
```

#### Scrape Multiple URLs from File

Create a file with URLs (one per line):

```bash
echo "https://www.cocinadominicana.com/batata-asada" > urls.txt
echo "https://www.cocinadominicana.com/mangu" >> urls.txt

make scrape-list FILE=urls.txt
python cli.py scrape-list urls.txt --output batch
```

#### Discover URLs Without Scraping

Preview what URLs would be scraped from a page:

```bash
make scrape-discover URL="https://www.cocinadominicana.com/inicia"
python cli.py discover "https://www.cocinadominicana.com/inicia"
python cli.py discover "https://www.cocinadominicana.com/inicia" --save discovered.txt
```

### CLI Commands Reference

| Command | Description | Example |
|---------|-------------|---------|
| `scrape-all` | Scrape all configured sections | `python cli.py scrape-all` |
| `scrape <url>` | Scrape single URL | `python cli.py scrape <url>` |
| `crawl <url>` | Crawl category and scrape articles | `python cli.py crawl <url> --depth 2` |
| `scrape-list <file>` | Scrape URLs from file | `python cli.py scrape-list urls.txt` |
| `discover <url>` | Discover URLs without scraping | `python cli.py discover <url>` |

### Programmatic Usage

```python
from scraper import Scraper

scraper = Scraper()

scraper.scrape_all_sections()

scraper.scrape_section("cultura_origenes")

article_data = scraper.scrape_url("https://www.cocinadominicana.com/article-url")

result = scraper.crawl_category(
    category_url="https://www.cocinadominicana.com/cocina",
    category_name="cocina",
    max_depth=2
)

urls = scraper.auto_discover_urls("https://www.cocinadominicana.com/inicia")
```

### Configuration

Edit `config.yaml` to customize scraping behavior:

```yaml
base_url: "https://www.cocinadominicana.com"
output_dir: "scraped_content"

filters:
  include_patterns:
    - "cocinadominicana\\.com/.*"
  exclude_patterns:
    - "wp-content"
    - "\\.(jpg|png|gif)$"

crawler:
  max_depth: 2
  delay_seconds: 0.5
  skip_existing: true
```

## Output Structure

```
scraped_content/
â”œâ”€â”€ cultura_origenes/
â”‚   â”œâ”€â”€ article-title.md
â”‚   â”œâ”€â”€ article-title.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tradiciones_costumbres/
â”œâ”€â”€ festividades_celebraciones/
â”œâ”€â”€ comparaciones/
â”œâ”€â”€ cocina/
â”‚   â”œâ”€â”€ crawl_summary.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ custom/
â”œâ”€â”€ batch/
â””â”€â”€ scraping_summary.json
```

### File Formats

**Markdown Files (`.md`)**

```markdown
---
title: "Article Title"
description: "Article description"
url: https://www.cocinadominicana.com/article-url
scraped_at: 2024-10-14T14:15:30.123456
word_count: 450
char_count: 2847
---

# Article Content
Article markdown content here...
```

**JSON Metadata (`.json`)**

```json
{
  "title": "Article Title",
  "description": "Article description", 
  "url": "https://www.cocinadominicana.com/article-url",
  "url_slug": "article_url_slug",
  "scraped_at": "2024-10-14T14:15:30.123456",
  "word_count": 450,
  "char_count": 2847
}
```

**Summary Report (`scraping_summary.json`)**

```json
{
  "scraping_session": {
    "start_time": "2024-10-14T14:15:30.123456",
    "end_time": "2024-10-14T14:45:30.123456", 
    "duration_seconds": 1800.0,
    "total_articles_scraped": 127,
    "total_sections_failed": 0
  },
  "sections": {
    "cultura_origenes": {
      "name": "Cultura y OrÃ­genes",
      "url": "https://www.cocinadominicana.com/cultura/herencia",
      "articles_scraped": 42,
      "directory": "cultura_origenes"
    }
  }
}
```

## Performance Tuning

### Worker Configuration

Edit `firecrawl.env` to adjust scraping speed:

```bash
NUM_WORKERS_PER_QUEUE=8
```

| Workers | Speed | Resource Usage | Use Case |
|---------|-------|----------------|----------|
| 4 | Slow | Low | Limited resources |
| 8 | Normal | Medium | Default, balanced |
| 16 | Fast | High | Good hardware |
| 32 | Very Fast | Very High | Server-grade |

After changes:
```bash
docker-compose restart firecrawl-api
```

### Performance Comparison

| Metric | Cloud API | Local Setup |
|--------|-----------|-------------|
| Rate Limits | Yes (strict) | No |
| Cost | Paid/Limited free | Free |
| Speed per article | ~3 seconds | ~0.5 seconds |
| Setup time | Instant | 5 minutes |
| Control | Limited | Full |

## Logging and Monitoring

### Canonical Log Pattern

The scraper implements structured logging with canonical log lines for excellent observability:

**Session Tracking**

```
2024-10-14T14:15:30 [INFO] scraper: firecrawl_initialized api_url="http://localhost:3002"
2024-10-14T14:15:30 [INFO] scraper: scrape_session_started start_time="2024-10-14T14:15:30" sections_count=4
2024-10-14T14:45:30 [INFO] scraper: scrape_session_completed duration_seconds=1800 total_articles_scraped=127
```

**Performance Monitoring**

```
2024-10-14T14:16:15 [INFO] scraper: article_scrape_completed url="https://..." title="Recipe Title" word_count=450 duration_ms=890
2024-10-14T14:20:30 [INFO] scraper: section_processing_completed section="Cultura y OrÃ­genes" articles_scraped=32 duration_ms=45000
```

**Docker Service Logs**

```bash
docker-compose logs -f firecrawl-api

docker-compose logs -f redis

docker-compose logs -f postgres
```

### Log Files

- **Console Output**: Real-time logging to terminal
- **File Logging**: Persistent logs saved to `scraping.log`
- **Structured Format**: Machine-readable logs for monitoring tools
- **Docker Logs**: Service logs via `docker-compose logs`

## Reliability

### Error Recovery

- 3 retry attempts for failed requests
- Graceful handling of network timeouts
- Comprehensive error logging with context
- Resume capability for interrupted sessions

### Performance Characteristics

| Metric | Value |
|--------|-------|
| Scraping Speed | ~0.5 seconds per article |
| Memory Usage | <50MB for typical sessions |
| Success Rate | >95% with retry logic |
| Resume Efficiency | 100% skip rate for existing files |

## Development

### Code Architecture

The codebase follows Clean Code principles:

- **Single Responsibility**: Each function has one clear purpose
- **Function Size**: All functions â‰¤20 lines
- **Descriptive Naming**: Self-documenting code with minimal comments
- **Separation of Concerns**: Clear boundaries between operations

### Key Components

**Core Classes**

- `Scraper`: Main scraper orchestration
- `PerformanceTimer`: Context manager for operation timing

**Utility Functions**

- `setup_canonical_logger()`: Logger configuration
- `log_canonical()`: Structured logging implementation
- `create_safe_filename()`: Safe filename generation
- `save_json_file()`: JSON file operations

### Testing

```bash
# Validate syntax
python3 -m py_compile scraper.py utils.py

# Test import
python3 -c "from scraper import Scraper; print('Import successful')"

# Dry run (test configuration)
python3 -c "
from scraper import Scraper
scraper = Scraper()
print(f'Configured for {len(scraper.sections)} sections')
print(f'Output directory: {scraper.output_dir}')
"
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Follow Clean Code principles (functions â‰¤20 lines)
4. Add canonical logging for new operations
5. Test your changes thoroughly
6. Commit with conventional commit format
7. Push and create a Pull Request

### Commit Format

```bash
git commit -m "feat: add new scraping section

- Implement new section discovery logic
- Add canonical logging for section operations  
- Include performance metrics tracking"
```

## Troubleshooting

### Common Issues

**Services Won't Start**

```bash
docker-compose logs -f

docker-compose down -v && docker-compose up -d
```

**API Not Responding**

```bash
curl http://localhost:3002/test

docker-compose restart firecrawl-api
```

**Port Already in Use**

Edit `docker-compose.yml` and change port mappings:
```yaml
ports:
  - "3003:3002"
```

**Import Errors**

```
ModuleNotFoundError: No module named 'firecrawl'
```

**Solution**: Install dependencies with `pip install -r requirements.txt`

**For detailed troubleshooting, see `local-setup.md`**

### Debug Mode

Enable verbose logging:

```python
from utils import setup_canonical_logger

logger = setup_canonical_logger(__name__, level="DEBUG")
```

### Performance Analysis

Monitor performance with log analysis:

```bash
rg "duration_ms" scraping.log | tail -20

rg -c "scrape_success" scraping.log
rg -c "scrape_failed" scraping.log

docker stats
```

## License

This project is for educational and research purposes. Please respect the website's robots.txt and terms of service.

## Support

- **Issues**: [GitHub Issues](https://github.com/cristiandlahoz/cocina-dominicana-crawl/issues)
- **Documentation**: This README
- **Logs**: Check `scraping.log` for detailed operation logs

---

## Acknowledgment

This project has been partially supported by the Ministerio de EducaciÃ³n Superior, Ciencia y TecnologÃ­a (MESCyT) of the Dominican Republic through the FONDOCYT grant. The authors gratefully acknowledge this support.

Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of MESCyT.

---

**Built for preserving Dominican culinary culture**
