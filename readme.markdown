# Dominican Culinary Culture Scraper ðŸ‡©ðŸ‡´

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![API](https://img.shields.io/badge/Firecrawl-API-FF6B6B?style=for-the-badge&logo=fire&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Production-success?style=for-the-badge)

**Production-Ready Web Scraper for Dominican Culinary Heritage**

*Comprehensive content extraction from Cocina Dominicana with canonical logging, clean architecture, and robust error handling*

</div>

## Platform Requirements

> **This application is designed for Python 3.8+ environments with Firecrawl API access**

### System Architecture

```mermaid
graph TB
    A[scraper.py] --> B[Firecrawl API]
    A --> C[File System]
    B --> D[Content Extraction]
    C --> E[scraped_content/]
    E --> F[Markdown Files]
    E --> G[JSON Metadata]
    E --> H[Summary Report]
    
    D --> I[Rate Limiting]
    D --> J[Error Handling]
    I --> K[Exponential Backoff]
    J --> L[Retry Logic]
    
    style A fill:#3776AB,stroke:#fff,stroke-width:2px,color:#fff
    style B fill:#FF6B6B,stroke:#fff,stroke-width:2px,color:#fff
    style E fill:#28a745,stroke:#fff,stroke-width:2px,color:#fff
```

## Dependencies

### Core Requirements

| Component | Version | Purpose |
|-----------|---------|---------|
| Python | 3.8+ | Runtime environment |
| firecrawl-py | Latest | API client library |
| python-dotenv | Latest | Environment variable management |

### Firecrawl API
The application uses Firecrawl for intelligent content extraction:

| Feature | Description |
|---------|-------------|
| API Endpoint | `https://api.firecrawl.dev` |
| Authentication | API key via environment variable |
| Rate Limiting | Automatic detection and handling |
| Free Tier | Available at https://firecrawl.dev |

### Compatibility Matrix

```mermaid
gantt
    title Platform Support
    dateFormat X
    axisFormat %s
    
    section Operating Systems
    macOS     : 0, 1
    Linux     : 0, 1  
    Windows   : 0, 1
    
    section Python Versions
    Python 3.8  : 0, 1
    Python 3.9  : 0, 1
    Python 3.10 : 0, 1
    Python 3.11 : 0, 1
    Python 3.12 : 0, 1
```

## Installation & Setup

### Quick Start

```bash
git clone https://github.com/cristiandlahoz/cocina-dominicana-crawl.git
cd cocina-dominicana-crawl
```

### Installation Flow

```mermaid
flowchart LR
    A[Clone Repository] --> B[Create Virtual Environment]
    B --> C[Install Dependencies]
    C --> D[Configure Environment]
    D --> E[Run Scraper]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#9f9,stroke:#333,stroke-width:2px
```

#### Step 1: Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

#### Step 3: Configure Environment Variables

```bash
cp .env.example .env
# Edit .env and add your Firecrawl API key
```

**`.env` Configuration:**

```bash
# Firecrawl API Key (required)
# Get your free API key from https://firecrawl.dev
FIRECRAWL_API_KEY=your_api_key_here
```

## Features

Production-ready scraper with enterprise-grade reliability:

### Core Capabilities

- **Canonical Logging**: Structured logging following Stripe's canonical log pattern
- **Clean Architecture**: Functions follow Single Responsibility Principle with <20 line limit
- **Rate Limiting**: Intelligent retry logic with exponential backoff
- **Resume Capability**: Automatically skips already scraped articles
- **Performance Monitoring**: Built-in timing for all operations
- **Robust Error Handling**: Comprehensive exception handling with detailed logging
- **File Organization**: Automatic directory structure creation and safe filename generation

### Content Categories

The scraper targets 4 main cultural sections:

1. **Cultura y OrÃ­genes** (`cultura_origenes/`)
2. **Tradiciones y Costumbres** (`tradiciones_costumbres/`)  
3. **Festividades y Celebraciones** (`festividades_celebraciones/`)
4. **Comparaciones** (`comparaciones/`)

### Data Model

```mermaid
erDiagram
    ARTICLE {
        string title
        string description
        string url
        string url_slug
        datetime scraped_at
        int word_count
        int char_count
    }
    
    SECTION {
        string name
        string url
        string directory
        int articles_count
    }
    
    SESSION {
        datetime start_time
        datetime end_time
        float duration_seconds
        int total_articles
    }
    
    SECTION ||--o{ ARTICLE : contains
    SESSION ||--o{ SECTION : processes
```

## Usage

### Basic Usage

```bash
python scraper.py
```

### Programmatic Usage

```python
from scraper import Scraper

scraper = Scraper()

scraper.scrape_all_sections()

scraper.scrape_section("cultura_origenes")

article_data = scraper.scrape_article("https://www.cocinadominicana.com/article-url")
```

### Advanced Configuration

```python
from scraper import Scraper
from utils import setup_canonical_logger

logger = setup_canonical_logger(__name__, level="DEBUG")

scraper = Scraper()
scraper.scrape_all_sections()
```

## Output Structure

```
scraped_content/
â”œâ”€â”€ cultura_origenes/
â”‚   â”œâ”€â”€ article-title.md
â”‚   â”œâ”€â”€ article-title.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tradiciones_costumbres/
â”œâ”€â”€ festividades_celebraciones/
â”œâ”€â”€ comparaciones/
â””â”€â”€ scraping_summary.json
```

### File Formats

**Markdown Files (`.md`)**

```markdown
---
title: "Article Title"
description: "Article description"
url: https://www.cocinadominicana.com/article-url
scraped_at: 2024-10-14T14:15:30.123456
word_count: 450
char_count: 2847
---

# Article Content
Article markdown content here...
```

**JSON Metadata (`.json`)**

```json
{
  "title": "Article Title",
  "description": "Article description", 
  "url": "https://www.cocinadominicana.com/article-url",
  "url_slug": "article_url_slug",
  "scraped_at": "2024-10-14T14:15:30.123456",
  "word_count": 450,
  "char_count": 2847
}
```

**Summary Report (`scraping_summary.json`)**

```json
{
  "scraping_session": {
    "start_time": "2024-10-14T14:15:30.123456",
    "end_time": "2024-10-14T14:45:30.123456", 
    "duration_seconds": 1800.0,
    "total_articles_scraped": 127,
    "total_sections_failed": 0
  },
  "sections": {
    "cultura_origenes": {
      "name": "Cultura y OrÃ­genes",
      "url": "https://www.cocinadominicana.com/cultura/herencia",
      "articles_scraped": 42,
      "directory": "cultura_origenes"
    }
  }
}
```

## Logging and Monitoring

### Canonical Log Pattern

The scraper implements structured logging with canonical log lines for excellent observability:

**Session Tracking**

```
2024-10-14T14:15:30 [INFO] scraper: scrape_session_started start_time="2024-10-14T14:15:30" sections_count=4 output_dir="/path/to/scraped_content"
2024-10-14T14:45:30 [INFO] scraper: scrape_session_completed duration_seconds=1800 total_articles_scraped=127 total_sections_failed=0
```

**Performance Monitoring**

```
2024-10-14T14:16:15 [INFO] scraper: article_scrape_completed url="https://..." title="Recipe Title" word_count=450 duration_ms=890
2024-10-14T14:20:30 [INFO] scraper: section_processing_completed section="Cultura y OrÃ­genes" articles_scraped=32 duration_ms=45000
```

**Error Handling**

```
2024-10-14T14:17:45 [INFO] scraper: rate_limit_encountered wait_time=25 original_delay=20
2024-10-14T14:18:10 [INFO] scraper: scrape_retry url="https://..." attempt=2 max_retries=3
```

### Log Files

- **Console Output**: Real-time logging to terminal
- **File Logging**: Persistent logs saved to `scraping.log`
- **Structured Format**: Machine-readable logs for monitoring tools

## Rate Limiting and Reliability

### Performance Optimization Flow

```mermaid
flowchart TD
    A[Request Article] --> B{File Exists?}
    B -->|Yes| C[Skip - Resume]
    B -->|No| D[Scrape Content]
    
    D --> E{Rate Limited?}
    E -->|Yes| F[Wait + Buffer]
    E -->|No| G[Process Content]
    
    F --> H[Retry Request]
    H --> D
    
    G --> I{Success?}
    I -->|No| J{Retry < 3?}
    J -->|Yes| K[Exponential Backoff]
    K --> D
    J -->|No| L[Log Failure]
    
    I -->|Yes| M[Save Files]
    
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style M fill:#9f9,stroke:#333,stroke-width:2px
    style L fill:#f99,stroke:#333,stroke-width:2px
```

### Intelligent Rate Limiting

- Automatic detection of rate limit responses
- Dynamic wait times based on API responses
- 5-second buffer added to all delays
- Exponential backoff for retry attempts

### Error Recovery

- 3 retry attempts for failed requests
- Graceful handling of network timeouts
- Comprehensive error logging with context
- Resume capability for interrupted sessions

### Performance Characteristics

| Metric | Value |
|--------|-------|
| Scraping Speed | ~3 seconds per article (including rate limiting) |
| Memory Usage | <50MB for typical sessions |
| Success Rate | >95% with retry logic |
| Resume Efficiency | 100% skip rate for existing files |

## Development

### Code Architecture

The codebase follows Clean Code principles:

- **Single Responsibility**: Each function has one clear purpose
- **Function Size**: All functions â‰¤20 lines
- **Descriptive Naming**: Self-documenting code with minimal comments
- **Separation of Concerns**: Clear boundaries between operations

### Key Components

**Core Classes**

- `Scraper`: Main scraper orchestration
- `PerformanceTimer`: Context manager for operation timing

**Utility Functions**

- `setup_canonical_logger()`: Logger configuration
- `log_canonical()`: Structured logging implementation
- `create_safe_filename()`: Safe filename generation
- `save_json_file()`: JSON file operations

### Testing

```bash
# Validate syntax
python3 -m py_compile scraper.py utils.py

# Test import
python3 -c "from scraper import Scraper; print('Import successful')"

# Dry run (test configuration)
python3 -c "
from scraper import Scraper
scraper = Scraper()
print(f'Configured for {len(scraper.sections)} sections')
print(f'Output directory: {scraper.output_dir}')
"
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Follow Clean Code principles (functions â‰¤20 lines)
4. Add canonical logging for new operations
5. Test your changes thoroughly
6. Commit with conventional commit format
7. Push and create a Pull Request

### Commit Format

```bash
git commit -m "feat: add new scraping section

- Implement new section discovery logic
- Add canonical logging for section operations  
- Include performance metrics tracking"
```

## Troubleshooting

### Common Issues

**API Key Error**

```
ValueError: FIRECRAWL_API_KEY not found in environment
```

**Solution**: Ensure `.env` file exists with valid `FIRECRAWL_API_KEY`

**Rate Limiting**

```
rate_limit_encountered wait_time=25 original_delay=20
```

**Solution**: This is normal behavior - the scraper automatically handles rate limits

**Import Errors**

```
ModuleNotFoundError: No module named 'firecrawl'
```

**Solution**: Install dependencies with `pip install -r requirements.txt`

### Debug Mode

Enable verbose logging:

```python
from utils import setup_canonical_logger

logger = setup_canonical_logger(__name__, level="DEBUG")
```

### Performance Analysis

Monitor performance with log analysis:

```bash
# Analyze timing patterns
grep "duration_ms" scraping.log | tail -20

# Check success rates  
grep -c "scrape_success" scraping.log
grep -c "scrape_failed" scraping.log
```

## License

This project is for educational and research purposes. Please respect the website's robots.txt and terms of service.

## Support

- **Issues**: [GitHub Issues](https://github.com/cristiandlahoz/cocina-dominicana-crawl/issues)
- **Documentation**: This README
- **Logs**: Check `scraping.log` for detailed operation logs

---

## Acknowledgment

This project has been partially supported by the Ministerio de EducaciÃ³n Superior, Ciencia y TecnologÃ­a (MESCyT) of the Dominican Republic through the FONDOCYT grant. The authors gratefully acknowledge this support.

Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of MESCyT.

---

**Built for preserving Dominican culinary culture**
